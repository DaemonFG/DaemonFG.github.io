---
title: 模型评估方法
top: false
cover: false
toc: true
mathjax: true
tags: Model Evaluation
categories: Machine Learning
abbrlink: f8522835
date: 2019-11-08 11:56:27
password:
summary:
---

在机器学习和深度学习的过程中，工程师完成模型训练，必须通过模型评估才能上线，那么在什么场景下该使用怎样的模型评估方法呢？本文奖一一介绍。

首先明确一下几个表示(以二分类为例)：

- True Positive(TP)：将正样本预测为正类。
- True Negative(TN)：将负样本预测为负类。
- False Positive(FP)：将负样本预测为正类。
- False Negative(FN)：将正样本预测为负类。

|       | Positive | Negative |
| ----- | :------: | :------: |
| True  |    TP    |    TN    |
| False |    FP    |    FN    |

### 准确率(Accuracy)

$$
Acc = \frac{被预测对的正样本和负样本数}{所有样本数} = \frac{TP + FN}{TP + FP + FN + TN}
$$

#### 存在问题

从上述公式可以看出，加假如训练集中的正负样本不均衡时，例如：

1. 100个样本中有90个正样本和10个负样本
2. 100个样本中有50个正样本和50个负样本

假设正样本有90%被正确预测，负样本有50%被正确预测，那么：

1. $Acc = \frac{81 + 5}{100} = 86%$
2. $Acc = \frac{45 + 25}{100} = 70%$

可见训练集中的正负样本不均衡时，准确率非常不适用。由此我们需要引入新的评估指标精确率(Precision)和召回率(Recall)。

### 精确率(Precision)与召回率(Recall)

$$
Pre = \frac{被预测对的正样本数}{所有被预测为正样本数} = \frac{TP}{TP + FP}
$$

$$
Recall = \frac{被预测对的正样本数}{所有的正样本数} = \frac{TP}{TP + FN}
$$

#### 存在问题

从公式中看出，精确率和召回率分子相同，分母为$FP$和$FN$的差别，假设模型预测能力稳定，则$FN + FP$是一个定值，要提高召回率则要减少$FN$，那么相应的就要减少$FP$，那么损失的就是精确率。

精确率和召回率互斥关联，所以工业界为了寻求两个指标的平衡，往往会采用F1-score。

### F1-score

$$
F1 = \frac{2TP}{2TP + FN + FP} = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
$$

$F1$的意义就是将准确率和召回率这两个分值合并为一个分值。可以看到，$Recall$体现了分类模型对正样本的识别能力，$Recall$越高，说明模型对正样本的识别能力越强，$Precision$体现了模型对负样本的区分能力，$Precision$越高，说明模型对负样本的区分能力越强。F1-score是两者的综合。F1-score越高，说明分类模型越稳健。